{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuzbK_9AI9-b"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wm8GMvlPI9-c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For text processing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# For model building and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# For handling sparse matrices\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# For scaling numeric features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVxv9jx-I9-d"
      },
      "source": [
        "# Loading the Files\n",
        "\n",
        "Download the csv files into the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yVnUKRv_I9-e",
        "outputId": "99be037f-6559-4de8-b1b3-7a07151c4fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full train data shape: (1697533, 9)\n",
            "Test data shape: (212192, 2)\n",
            "Sampled train data shape: (848766, 9)\n",
            "First few rows of sampled training data:\n",
            "        Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
            "0   565351  B000053V8F  A3GAGA790C3F90                     0   \n",
            "1   450021  6305126062  A2U9M4SE42KFK8                     0   \n",
            "2  1124790  B00105308I  A1AISPOIIHTHXX                     0   \n",
            "3  1521546  B006WQUL64  A2435XASRUN5O0                     1   \n",
            "4   749431  B00008YGRS  A3I7UACA67A3NP                     0   \n",
            "\n",
            "   HelpfulnessDenominator        Time  \\\n",
            "0                       1  1370304000   \n",
            "1                       0  1379894400   \n",
            "2                       0  1206403200   \n",
            "3                       1  1374364800   \n",
            "4                       1  1389139200   \n",
            "\n",
            "                                             Summary  \\\n",
            "0              Technically Competent, but Lacks Soul   \n",
            "1                              Redford story telling   \n",
            "2       4.5; Laughing at the outrageous and shocking   \n",
            "3                     You Can't Get Away With Murder   \n",
            "4  S02E20 Anthology Of Interest 1 (Nichelle Nicho...   \n",
            "\n",
            "                                                Text  Score  \n",
            "0  While this follows the book much more closely ...    3.0  \n",
            "1  Great story on how issues can be over come wit...    NaN  \n",
            "2  Despite the fact that South Park has maintaine...    4.0  \n",
            "3  If you love Bogey, you'll be well entertained ...    NaN  \n",
            "4  Can I get my half hour back? Can I get my half...    NaN  \n",
            "Missing values in sampled training data:\n",
            "Id                             0\n",
            "ProductId                      0\n",
            "UserId                         0\n",
            "HelpfulnessNumerator           0\n",
            "HelpfulnessDenominator         0\n",
            "Time                           0\n",
            "Summary                       18\n",
            "Text                          33\n",
            "Score                     106122\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV files into DataFrames\n",
        "train_df_full = pd.read_csv('./data/train.csv')\n",
        "test_df = pd.read_csv('./data/test.csv')\n",
        "print(\"Full train data shape:\", train_df_full.shape)\n",
        "print(\"Test data shape:\", test_df.shape)\n",
        "\n",
        "# Sampling 50% of the Training Data\n",
        "train_df = train_df_full.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
        "print(\"Sampled train data shape:\", train_df.shape)\n",
        "\n",
        "# Display the first few rows of the sampled training data\n",
        "print(\"First few rows of sampled training data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Check for missing values in the sampled training data\n",
        "print(\"Missing values in sampled training data:\")\n",
        "print(train_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twBgHqZfI9-e"
      },
      "source": [
        "# Adding Features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0SDgxFQmI9-e"
      },
      "outputs": [],
      "source": [
        "def add_features_to(df):\n",
        "    \"\"\"\n",
        "    Adds new features to the DataFrame and handles missing values.\n",
        "    \"\"\"\n",
        "    # Handle missing values in 'HelpfulnessDenominator' to avoid division by zero\n",
        "    df['HelpfulnessDenominator'] = df['HelpfulnessDenominator'].replace(0, np.nan)\n",
        "    df['Helpfulness'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator']\n",
        "    df['Helpfulness'] = df['Helpfulness'].fillna(0)\n",
        "\n",
        "    # Convert 'Time' to datetime and extract date components\n",
        "    df['Time'] = pd.to_datetime(df['Time'], unit='s')\n",
        "    df['Review_Year'] = df['Time'].dt.year\n",
        "    df['Review_Month'] = df['Time'].dt.month\n",
        "    df['Review_Day'] = df['Time'].dt.day\n",
        "\n",
        "    # Fill NaN values in 'Summary' and 'Text'\n",
        "    df['Summary'] = df['Summary'].fillna('')\n",
        "    df['Text'] = df['Text'].fillna('')\n",
        "\n",
        "    # Calculate the length of 'Summary' and 'Text'\n",
        "    df['Summary_length'] = df['Summary'].apply(len)\n",
        "    df['Text_length'] = df['Text'].apply(len)\n",
        "\n",
        "    # Calculate word counts in 'Summary' and 'Text'\n",
        "    df['Summary_word_count'] = df['Summary'].apply(lambda x: len(x.split()))\n",
        "    df['Text_word_count'] = df['Text'].apply(lambda x: len(x.split()))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply feature engineering to the training data\n",
        "train_df = add_features_to(train_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zptr12ZHI9-f"
      },
      "source": [
        "# Preparing Training and Test Data Model Building and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DGDA5qEDI9-g"
      },
      "outputs": [],
      "source": [
        "# Training data: Rows where 'Score' is not null\n",
        "train_data = train_df[train_df['Score'].notnull()].copy()\n",
        "\n",
        "# Test data: Merge test_df with train_df_full to get all necessary data\n",
        "test_data = pd.merge(test_df[['Id']], train_df_full.drop(columns=['Score']), on='Id', how='left')\n",
        "\n",
        "# Combine 'Summary' and 'Text' into 'Combined_Text' for both train and test data\n",
        "train_data['Combined_Text'] = train_data['Summary'] + ' ' + train_data['Text']\n",
        "test_data['Combined_Text'] = test_data['Summary'] + ' ' + test_data['Text']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK-zxj03I9-g"
      },
      "source": [
        "# Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2OqjH2iTI9-g",
        "outputId": "3fa35d2d-3154-4331-ed56-41379c412372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaN values in train_data['Combined_Text']: 0\n",
            "Number of NaN values in test_data['Combined_Text']: 0\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'Summary' and 'Text' columns have no NaN values\n",
        "train_data['Summary'] = train_data['Summary'].fillna('').astype(str)\n",
        "train_data['Text'] = train_data['Text'].fillna('').astype(str)\n",
        "test_data['Summary'] = test_data['Summary'].fillna('').astype(str)\n",
        "test_data['Text'] = test_data['Text'].fillna('').astype(str)\n",
        "\n",
        "# Create 'Combined_Text' column by concatenating 'Summary' and 'Text'\n",
        "train_data['Combined_Text'] = (train_data['Summary'] + ' ' + train_data['Text']).fillna('').astype(str)\n",
        "test_data['Combined_Text'] = (test_data['Summary'] + ' ' + test_data['Text']).fillna('').astype(str)\n",
        "\n",
        "# Check if any NaN values are present in the 'Combined_Text' column\n",
        "print(\"Number of NaN values in train_data['Combined_Text']:\", train_data['Combined_Text'].isna().sum())\n",
        "print(\"Number of NaN values in test_data['Combined_Text']:\", test_data['Combined_Text'].isna().sum())\n",
        "\n",
        "# Combine the text data for vectorization\n",
        "combined_text = pd.concat([train_data['Combined_Text'], test_data['Combined_Text']], axis=0)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1, 2), stop_words='english')\n",
        "\n",
        "# Fit and transform the combined text data\n",
        "tfidf_combined_text = tfidf_vectorizer.fit_transform(combined_text)\n",
        "\n",
        "# Split the vectorized data back into training and test sets\n",
        "tfidf_train = tfidf_combined_text[:len(train_data)]\n",
        "tfidf_test = tfidf_combined_text[len(train_data):]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lN_rlKbI9-g"
      },
      "source": [
        "#  Preparing Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tqd1Wi7-I9-g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing features in train_data: set()\n",
            "Missing features in test_data: set()\n",
            "X_train_numeric shape: (742644, 2)\n",
            "X_test_numeric shape: (212192, 2)\n"
          ]
        }
      ],
      "source": [
        "# Prepare Numerical Features\n",
        "# ----------------------------\n",
        "# List of numerical features to include\n",
        "numeric_features = [\n",
        "    'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Helpfulness',\n",
        "    'Summary_length', 'Text_length', 'Summary_word_count', 'Text_word_count',\n",
        "    'Review_Year', 'Review_Month', 'Review_Day'\n",
        "]\n",
        "\n",
        "# Ensure that all the numeric features are present in both train_data and test_data\n",
        "# Filter only the features that exist in both datasets\n",
        "numeric_features = [\n",
        "    feature for feature in numeric_features\n",
        "    if feature in train_data.columns and feature in test_data.columns\n",
        "]\n",
        "\n",
        "# Check which features are missing from the train and test data\n",
        "missing_train_features = set(numeric_features) - set(train_data.columns)\n",
        "missing_test_features = set(numeric_features) - set(test_data.columns)\n",
        "\n",
        "print(f\"Missing features in train_data: {missing_train_features}\")\n",
        "print(f\"Missing features in test_data: {missing_test_features}\")\n",
        "\n",
        "# Prepare numeric features for training and test data\n",
        "X_train_numeric = train_data[numeric_features].fillna(0)\n",
        "X_test_numeric = test_data[numeric_features].fillna(0)\n",
        "\n",
        "# Verify that X_train_numeric and X_test_numeric are defined and have the expected shape\n",
        "print(\"X_train_numeric shape:\", X_train_numeric.shape)\n",
        "print(\"X_test_numeric shape:\", X_test_numeric.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combining Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tfidf_train: (742644, 8000)\n",
            "Shape of X_train_num_sparse: (742644, 2)\n",
            "Shape of tfidf_test: (212192, 8000)\n",
            "Shape of X_test_num_sparse: (212192, 2)\n",
            "X_train_combined shape: (742644, 8002)\n",
            "X_test_combined shape: (212192, 8002)\n",
            "y_train shape: (742644,)\n"
          ]
        }
      ],
      "source": [
        "# Combining Features\n",
        "# ------------------\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Convert numeric features to sparse matrices\n",
        "# Ensure that X_train_numeric and X_test_numeric are correctly prepared\n",
        "X_train_num_sparse = csr_matrix(X_train_numeric.values)\n",
        "X_test_num_sparse = csr_matrix(X_test_numeric.values)\n",
        "\n",
        "# Combine TF-IDF text features with numeric features\n",
        "# Ensure that `tfidf_train` and `tfidf_test` were prepared correctly during text vectorization\n",
        "print(f\"Shape of tfidf_train: {tfidf_train.shape}\")\n",
        "print(f\"Shape of X_train_num_sparse: {X_train_num_sparse.shape}\")\n",
        "print(f\"Shape of tfidf_test: {tfidf_test.shape}\")\n",
        "print(f\"Shape of X_test_num_sparse: {X_test_num_sparse.shape}\")\n",
        "\n",
        "# Ensure the shapes of the TF-IDF and numeric matrices are compatible before combining\n",
        "X_train_combined = hstack([tfidf_train, X_train_num_sparse])\n",
        "X_test_combined = hstack([tfidf_test, X_test_num_sparse])\n",
        "\n",
        "# Check the final shapes after combining\n",
        "print(f\"X_train_combined shape: {X_train_combined.shape}\")\n",
        "print(f\"X_test_combined shape: {X_test_combined.shape}\")\n",
        "\n",
        "# Define the target variable (make sure it exists in train_data)\n",
        "y_train = train_data['Score']\n",
        "\n",
        "# Verify that y_train has the correct shape\n",
        "print(f\"y_train shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Building and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression model...\n",
            "Logistic Regression Accuracy: 0.6562983071296611\n",
            "Training Multinomial Naive Bayes model...\n",
            "Multinomial Naive Bayes Accuracy: 0.5817430693575926\n",
            "Training Random Forest Classifier model...\n",
            "Random Forest Classifier Accuracy: 0.5968781812012216\n"
          ]
        }
      ],
      "source": [
        "# Split the training data into training and validation sets\n",
        "X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
        "    X_train_combined, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "# Logistic Regression Model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "logreg = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "logreg.fit(X_train_part, y_train_part)\n",
        "\n",
        "# Predict and evaluate Logistic Regression\n",
        "y_pred_logreg = logreg.predict(X_valid)\n",
        "accuracy_logreg = accuracy_score(y_valid, y_pred_logreg)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
        "\n",
        "# Multinomial Naive Bayes Model\n",
        "print(\"Training Multinomial Naive Bayes model...\")\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_part, y_train_part)\n",
        "\n",
        "# Predict and evaluate Multinomial Naive Bayes\n",
        "y_pred_mnb = mnb.predict(X_valid)\n",
        "accuracy_mnb = accuracy_score(y_valid, y_pred_mnb)\n",
        "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_mnb)\n",
        "\n",
        "# Random Forest Classifier Model\n",
        "print(\"Training Random Forest Classifier model...\")\n",
        "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf_classifier.fit(X_train_part, y_train_part)\n",
        "\n",
        "# Predict and evaluate Random Forest Classifier\n",
        "y_pred_rf = rf_classifier.predict(X_valid)\n",
        "accuracy_rf = accuracy_score(y_valid, y_pred_rf)\n",
        "print(\"Random Forest Classifier Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Accuracies:\n",
            "Logistic Regression Accuracy: 0.6562983071296611\n",
            "Multinomial Naive Bayes Accuracy: 0.5817430693575926\n",
            "Random Forest Classifier Accuracy: 0.5968781812012216\n",
            "\n",
            "Best model based on validation accuracy: Logistic Regression\n"
          ]
        }
      ],
      "source": [
        "# Compare model accuracies\n",
        "print(\"\\nModel Accuracies:\")\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_logreg}\")\n",
        "print(f\"Multinomial Naive Bayes Accuracy: {accuracy_mnb}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf}\")\n",
        "\n",
        "# Select the best model based on validation accuracy\n",
        "accuracies = {\n",
        "    'Logistic Regression': accuracy_logreg,\n",
        "    'Multinomial Naive Bayes': accuracy_mnb,\n",
        "    'Random Forest Classifier': accuracy_rf\n",
        "}\n",
        "\n",
        "best_model_name = max(accuracies, key=accuracies.get)\n",
        "print(f\"\\nBest model based on validation accuracy: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Model Training and Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Retraining the best model (Logistic Regression) on the entire training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Submission file created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Retrain the best model on the entire training data\n",
        "print(f\"\\nRetraining the best model ({best_model_name}) on the entire training data...\")\n",
        "\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_model = LogisticRegression(max_iter=5000, n_jobs=-1)\n",
        "    best_model.fit(X_train_combined, y_train)\n",
        "    y_test_pred = best_model.predict(X_test_combined)\n",
        "\n",
        "elif best_model_name == 'Multinomial Naive Bayes':\n",
        "    best_model = MultinomialNB()\n",
        "    best_model.fit(X_train_combined, y_train)\n",
        "    y_test_pred = best_model.predict(X_test_combined)\n",
        "\n",
        "elif best_model_name == 'Random Forest Classifier':\n",
        "    best_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "    best_model.fit(X_train_combined, y_train)\n",
        "    y_test_pred = best_model.predict(X_test_combined)\n",
        "else:\n",
        "    print(\"No valid model selected.\")\n",
        "    y_test_pred = None\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = test_df[['Id']].copy()\n",
        "submission['Score'] = y_test_pred\n",
        "\n",
        "# Save the submission file to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file created successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
